{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Computational Linguistics and how does it relate to NLP?**\n",
        "\n",
        "Computational Linguistics is the scientific study of language from a computational perspective, focusing on how human language can be modeled, analyzed, and understood using algorithms and formal rules. It provides the theoretical and linguistic foundations (such as syntax, semantics, and grammar) for Natural Language Processing (NLP), while NLP applies these theories practically to build systems like chatbots, translators, and speech recognition tools."
      ],
      "metadata": {
        "id": "N5Lx8xVDLuSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Briefly describe the historical evolution of Natural Language Processing.**\n",
        "\n",
        "Natural Language Processing evolved from rule-based approaches in the 1950s and 1960s, where language understanding relied on handcrafted grammar and linguistic rules. In the 1990s, the field shifted to statistical and machine learning methods using large text corpora. More recently, deep learning and transformer-based models have driven major advances, enabling highly accurate tasks such as translation, sentiment analysis, and conversational AI."
      ],
      "metadata": {
        "id": "5kkwpo3UL54J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: List and explain three major use cases of NLP in today’s tech industry.**\n",
        "\n",
        "**Chatbots and Virtual Assistants**: NLP enables systems like customer support bots and voice assistants to understand user queries, generate relevant responses, and carry out conversations in natural language.\n",
        "\n",
        "**Machine Translation**: NLP is used to automatically translate text or speech between languages (e.g., Google Translate), helping break language barriers in global communication.\n",
        "\n",
        "**Sentiment Analysis**: Companies use NLP to analyze customer reviews, social media posts, and feedback to understand opinions, emotions, and overall public sentiment toward products or brands."
      ],
      "metadata": {
        "id": "fpsUlI-TMC_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is text normalization and why is it essential in text processing tasks?**\n",
        "\n",
        "Text normalization is the process of cleaning and standardizing raw text by converting it into a consistent format, such as lowercasing text, removing punctuation, correcting spelling, and expanding abbreviations. It is essential because it reduces noise and variability in text data, helping NLP models process input more accurately and improving the performance of tasks like text classification, search, and information retrieval.\n"
      ],
      "metadata": {
        "id": "jsDGQ2_bMPOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare and contrast stemming and lemmatization with suitable**\n",
        "**examples**.\n",
        "\n",
        "Stemming and lemmatization are text preprocessing techniques used to reduce words to their base form, but they differ in accuracy and approach.\n",
        "\n",
        "Stemming removes word suffixes using simple rules, often producing non-dictionary words.\n",
        "\n",
        "Example: running, runner, runs → run\n",
        "\n",
        "Lemmatization uses vocabulary and grammatical analysis to return the meaningful base (dictionary) form of a word.\n",
        "\n",
        "Example: running → run, better → good\n",
        "\n",
        "In short, stemming is faster but less accurate, while lemmatization is slower but more linguistically correct."
      ],
      "metadata": {
        "id": "e2IYf-iSMZd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "# email addresses from the following block of text:\n",
        "# “Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "# our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "# via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n",
        "\n",
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regex pattern for email addresses\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all email addresses\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV17QyWHL26R",
        "outputId": "3c791633-8207-49f2-bf00-9bc8ba8a998f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "# frequency distribution using Python and NLTK:\n",
        "# “Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "# computer science, and artificial intelligence. It enables machines to understand,\n",
        "# interpret, and generate human language. Applications of NLP include chatbots,\n",
        "# sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "# in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"Frequency:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0V3FkViNa9C",
        "outputId": "6e6b2fc7-a612-493d-f425-339f15be009c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'it', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'applications', 'of', 'nlp', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'as', 'technology', 'advances', ',', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "Frequency:\n",
            "natural: 1\n",
            "language: 2\n",
            "processing: 1\n",
            "(: 1\n",
            "nlp: 3\n",
            "): 1\n",
            "is: 2\n",
            "a: 1\n",
            "fascinating: 1\n",
            "field: 1\n",
            "that: 1\n",
            "combines: 1\n",
            "linguistics: 1\n",
            ",: 7\n",
            "computer: 1\n",
            "science: 1\n",
            "and: 3\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            ".: 4\n",
            "it: 1\n",
            "enables: 1\n",
            "machines: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "interpret: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "applications: 1\n",
            "of: 2\n",
            "include: 1\n",
            "chatbots: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "as: 1\n",
            "technology: 1\n",
            "advances: 1\n",
            "the: 1\n",
            "role: 1\n",
            "in: 1\n",
            "modern: 1\n",
            "solutions: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "critical: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "# proper nouns in a given text.\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"proper noun\")\n",
        "for token in doc:\n",
        "  if token.pos_ == \"PROPN\":\n",
        "    print(f\"Token: {token.text} - POS Tag: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "watrKyuvRdjW",
        "outputId": "c5fcd837-21e1-4d87-f311-65a0c85cb1bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proper noun\n",
            "Token: Natural - POS Tag: PROPN\n",
            "Token: Language - POS Tag: PROPN\n",
            "Token: Processing - POS Tag: PROPN\n",
            "Token: NLP - POS Tag: PROPN\n",
            "Token: NLP - POS Tag: PROPN\n",
            "Token: NLP - POS Tag: PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "# following dataset consisting of example sentences:\n",
        "# dataset = [\n",
        "#  \"Natural language processing enables computers to understand human language\",\n",
        "#  \"Word embeddings are a type of word representation that allows words with similar\n",
        "# meaning to have similar representation\",\n",
        "#  \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "#  \"Text preprocessing is a critical step before training word embeddings\",\n",
        "#  \"Tokenization and normalization help clean raw text for modeling\"\n",
        "# ]\n",
        "# Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "# Gensim.\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# tokenize and preprocessing\n",
        "# preprocessing- lowercase, remove punctuation\n",
        "\n",
        "tokenized_data = [simple_preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences = tokenized_data,\n",
        "    vector_size=100,\n",
        "    window = 5,\n",
        "    min_count=1,\n",
        "    workers = 4\n",
        ")\n",
        "\n",
        "# get word vector\n",
        "word_vector = model.wv[\"language\"]\n",
        "\n",
        "#get similar words\n",
        "similar_words = model.wv.most_similar(\"word\")\n",
        "\n",
        "print(f\"word vector: {word_vector}\")\n",
        "print(f\"similar word: {similar_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng_JjInSXdMN",
        "outputId": "2d709439-b9db-46b1-80a9-a715669e3827"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "word vector: [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
            "similar word: [('training', 0.21893921494483948), ('for', 0.21613667905330658), ('popular', 0.19544260203838348), ('processing', 0.16989612579345703), ('have', 0.15211786329746246), ('allows', 0.142473042011261), ('with', 0.1090250238776207), ('embedding', 0.09935855120420456), ('that', 0.09629563987255096), ('help', 0.09322801232337952)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process,and extract useful insights using NLP techniques from thousands of customer reviews.**\n",
        "\n",
        "First, I would clean and preprocess the customer reviews by removing noise such as HTML tags, URLs, punctuation, and stopwords, converting text to lowercase, and applying tokenization and lemmatization. This step ensures the text is consistent and ready for analysis, reducing ambiguity and improving model performance.\n",
        "\n",
        "Next, I would analyze the processed text using NLP techniques like sentiment analysis to understand customer opinions and topic modeling to identify common themes or issues. I would then summarize and visualize these insights to help the fintech team make data-driven decisions and improve customer satisfaction."
      ],
      "metadata": {
        "id": "jk6njcqSp5sp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uehJhu_jpjza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}